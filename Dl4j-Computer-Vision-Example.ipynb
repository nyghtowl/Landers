{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL4J Neural Net Computer Vision Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets are a machine learning algorithm used for classificaiton and prediction that can deal with complex dimensionality. This notebook provides sample code on how to structure, run and save a neural net using DL4J for a simplified computer vision problem. There are pictures of different animals and the goal is to differentiate and classify them by giving probabilities of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn_diagram.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets are especially great for image and word datasets that are not dense. The data is convereted to a numerical representation and fed into the net where each node in the net applies a linear and non-linear transformation.\n",
    "\n",
    ">***linear equation***<br>\n",
    ">$z_k= \\sum_{j=1} \\mathbf{w_{k,j}}\\mathbf{x_j} + \\mathbf{b_k}$\n",
    "\n",
    "\n",
    ">***sigmoid non-linear equation***<br>\n",
    ">$y= \\sigma\\Bigg(\\dfrac{1}{(1+\\mathrm{e}^{-z})}\\Bigg)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights ($w$), also known as parameters, are used to fit the model to the objective/goal of the model. In order to accomplish this, gradient descent optimization techniques are used to ***find the optimal weights*** that will lead to correct classification. Gradient descent takes the derivative of the calculated model loss and shifts the weights using learning rates and other hyper parameters like momentum to move the weight up or down the gradient curve. More information about how gradient descent works can be found in the resource section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/Extrema_example.svg/600px-Extrema_example.svg.png\">\n",
    "\n",
    "<center>- Wikipedia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on DL4J and how neural nets function can be found at:\n",
    "- DL4J http://deeplearning4j.org/documentation.html\n",
    "- Neural Nets for Newbies https://youtu.be/Cu6A96TUy_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Java 8\n",
    "- Maven 3.3.9\n",
    "- iScala Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Below is for Jupyter-Scala notebook. If iScala is used then below should change to load dependencies\n",
    "load.resolver(\"DefaultMavenRepository\" at \"https://repo1.maven.org/maven2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdl4jVersion\u001b[0m: java.lang.String = \u001b[32m\"0.4-rc3.8\"\u001b[0m\n",
       "\u001b[36mnd4jVersion\u001b[0m: java.lang.String = \u001b[32m\"0.4-rc3.8\"\u001b[0m\n",
       "\u001b[36mcanovaVersion\u001b[0m: java.lang.String = \u001b[32m\"0.0.0.14\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val dl4jVersion = \"0.4-rc3.8\"\n",
    "val nd4jVersion = \"0.4-rc3.8\"\n",
    "val canovaVersion = \"0.0.0.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load.ivy(\"org.deeplearning4j\" % \"deeplearning4j-core\" % dl4jVersion)\n",
    "load.ivy(\"org.deeplearning4j\" % \"deeplearning4j-nlp\" % dl4jVersion)\n",
    "load.ivy(\"org.deeplearning4j\" % \"deeplearning4j-ui\" % dl4jVersion)\n",
    "load.ivy(\"org.nd4j\" % \"nd4j-x86\" % nd4jVersion)\n",
    "load.ivy(\"canova-spark\" % \"org.nd4j\" % canovaVersion)\n",
    "load.ivy(\"canova-nd4j-codec\" % \"org.nd4j\" % canovaVersion)\n",
    "load.ivy(\"canova-nd4j-image\" % \"org.nd4j\" % canovaVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.commons.io.{FileUtils, FilenameUtils}\n",
    "import org.canova.api.records.reader.RecordReader\n",
    "import org.canova.api.split.LimitFileSplit\n",
    "import org.canova.image.loader.BaseImageLoader\n",
    "import org.canova.image.recordreader.ImageRecordReader\n",
    "import org.deeplearning4j.datasets.canova.RecordReaderDataSetIterator\n",
    "import org.deeplearning4j.datasets.iterator.DataSetIterator\n",
    "import org.deeplearning4j.eval.Evaluation\n",
    "import org.deeplearning4j.nn.api.OptimizationAlgorithm\n",
    "import org.deeplearning4j.nn.conf.MultiLayerConfiguration\n",
    "import org.deeplearning4j.nn.conf.NeuralNetConfiguration\n",
    "import org.deeplearning4j.nn.conf.layers.ConvolutionLayer\n",
    "import org.deeplearning4j.nn.conf.layers.DenseLayer\n",
    "import org.deeplearning4j.nn.conf.layers.OutputLayer\n",
    "import org.deeplearning4j.nn.conf.layers.SubsamplingLayer\n",
    "import org.deeplearning4j.nn.conf.layers.LocalResponseNormalization\n",
    "import org.deeplearning4j.nn.multilayer.MultiLayerNetwork\n",
    "import org.deeplearning4j.nn.weights.WeightInit\n",
    "import org.deeplearning4j.nn.conf.GradientNormalization\n",
    "import org.deeplearning4j.nn.conf.Updater\n",
    "import org.deeplearning4j.optimize.listeners.ScoreIterationListener\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.dataset.{SplitTestAndTrain, DataSet}\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import org.nd4j.linalg.lossfunctions.LossFunctions\n",
    "import java.io.{FileOutputStream, DataOutputStream, IOException, File}\n",
    "import java.util.{Random}\n",
    "import scala.collection.mutable.ListBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to cleanup and load the data for training and testing.\n",
    "- Store the data in a folder that the model can load from\n",
    "- Confirm the formats are the same (e.g. pictures exist and have similar sizes)\n",
    "- Convert data to a DataSet structure (numerical feature format and labels)\n",
    "- Setup the data to load in batches inside an iterator\n",
    "\n",
    "Something to be aware of with data is supervised vs. unsupervised which just means labeled vs unlabeled. In this example we have labeled images we are working with. Thus, it's supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images provided in this example are from the U.S Fish and Wildlife Service because the images are in the public domain. There four categories with ~ 20 images each in the dataset provided:\n",
    "\n",
    "- bear\n",
    "- deer\n",
    "- duck\n",
    "- turtle\n",
    "\n",
    "The images vary in pixel size and they are all RGB which means they have 3 channels of color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>***Example Image***</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"animals/turtle/Blandings_Turtle.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Load images and labels\n",
    "val mainPath: File = new File(\"animals\")\n",
    "val labels: List[String] = List(\"bear\", \"deer\", \"duck\", \"turtle\")\n",
    "\n",
    "val recordReader: RecordReader = new ImageRecordReader(width, height, channels, appendLabels)\n",
    "try {\n",
    "  recordReader.initialize(\n",
    "    new LimitFileSplit(mainPath, BaseImageLoader.ALLOWED_FORMATS, numExamples, outputNum, null, new Random(123)))\n",
    "} catch {\n",
    "  case ioe: IOException => ioe.printStackTrace()\n",
    "  case e: InterruptedException => e.printStackTrace()\n",
    "}\n",
    "val dataIter: DataSetIterator = new RecordReaderDataSetIterator(recordReader, batchSize, -1, outputNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with computer vision, you will want many more examples to run through your model for it to build a solid representation of the different animals. The sample set is too small to achieve high accuracy scores. When you have sparse examples, use techniques to modify and expand the dataset such as:\n",
    "- flip images by various degrees\n",
    "- change the color saturation (including change to grey scale)\n",
    "- crop the image in different positions\n",
    "- search and download more examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model configuration takes experimentation to get familiar with all the options. Below outlines key attributes that you can define in the model configuration. \n",
    "\n",
    "- ***weightInit*** = how to initialize parameters which is typically a variation on random\n",
    "- ***activation*** = non-linear function applied to parameters (weights & bias) on every node in the layer\n",
    "- ***seed*** = locks parameter initialization each time for consistancy when checking hyper-parameters impact\n",
    "- ***gradientNormalization*** = regularization techniques to smooth gradient results\n",
    "- ***optimizationAlgo*** = type of convext optimizer used to calculate gradients that determines how to apply that loss function gradient to weight updates\n",
    "- ***updater*** = type of equation to use when updating parameters (e.g. Nesterovs applies momentum to the learning rate for the gradient update)\n",
    "- ***learningRate*** = the step to take down or up the optimizer algorithm to improve model convergence\n",
    "- ***regularization*** = tells the model to apply weight decay (e.g. l1 or l2 defines the amount to apply and this applied to both weights and bias)\n",
    "- ***list*** = how many layers are in the model and does not count input as a layer\n",
    "- ***layer*** = construct to define each layer. requires a number when there are more than one\n",
    "- ***backprop*** = whether to apply backprop to the model for parameters updates\n",
    "- ***pretrain*** = whether to pretrain the model\n",
    "\n",
    "Note, most of these can be defined globally or inside the definition of each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Variables*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val seed = 123\n",
    "val height = 120\n",
    "val width = 120\n",
    "val channels = 3\n",
    "val numExamples = 80\n",
    "val outputNum = 4\n",
    "val batchSize = 20\n",
    "val listenerFreq = 5\n",
    "val appendLabels = true\n",
    "val iterations = 2\n",
    "val epochs = 2\n",
    "val splitTrainNum = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Computer Vision Common Configuration***<br>\n",
    "Its good to start with common configuration approaches like the ones provided below and then use training and tunning to modify hyperparameters. More information on this topic is covered in the Tuning section. \n",
    "\n",
    "- ***\"relu\"*** = rectifed linear unit is an activation function that helps prevent gradient vanishing because its sets the activation threshold at zero\n",
    "> $f(x)=max(0,x)$\n",
    "- ***LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD*** *(aka cross-entropy)* = evaluates and scores model error\n",
    "> $H_y{'}(y) = -\\sum_{i} \\mathbf{y_i}log({y_i})$\n",
    "- ***OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT*** = how to update weights based on error and gradient from the full training set\n",
    "> $w = w -\\alpha(y_i-h_w(x_i))x_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tiny ImageNet Example***<br>\n",
    "Below are two different example configurations. First is pulled from the Tiny ImageNet paper that provides guidance on how to build as compact a model as possible to be effective in image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Tiny ImageNet Example\n",
    "val confTiny: MultiLayerConfiguration = new NeuralNetConfiguration.Builder()\n",
    "  .seed(seed)\n",
    "  .iterations(iterations)\n",
    "  .activation(\"relu\")\n",
    "  .weightInit(WeightInit.XAVIER)\n",
    "  .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer)\n",
    "  .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "  .updater(Updater.NESTEROVS)\n",
    "  .learningRate(0.01)\n",
    "  .momentum(0.9)\n",
    "  .regularization(true)\n",
    "  .l2(0.04)\n",
    "  .useDropConnect(true)\n",
    "  .list()\n",
    "  .layer(0, new ConvolutionLayer.Builder(5, 5)\n",
    "    .name(\"cnn1\")\n",
    "    .nIn(channels)\n",
    "    .stride(1, 1)\n",
    "    .padding(2, 2)\n",
    "    .nOut(32)\n",
    "    .build())\n",
    "  .layer(1, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n",
    "    .kernelSize(3, 3)\n",
    "    .name(\"pool1\")\n",
    "    .build())\n",
    "  .layer(2, new LocalResponseNormalization.Builder(3, 5e-05, 0.75).build())\n",
    "  .layer(3, new ConvolutionLayer.Builder(5, 5)\n",
    "    .name(\"cnn2\")\n",
    "    .stride(1, 1)\n",
    "    .padding(2, 2)\n",
    "    .nOut(32)\n",
    "    .build())\n",
    "  .layer(4, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n",
    "    .kernelSize(3, 3)\n",
    "    .name(\"pool2\")\n",
    "    .build())\n",
    "  .layer(5, new LocalResponseNormalization.Builder(3, 5e-05, 0.75).build())\n",
    "  .layer(6, new ConvolutionLayer.Builder(5, 5)\n",
    "    .name(\"cnn3\")\n",
    "    .stride(1, 1)\n",
    "    .padding(2, 2)\n",
    "    .nOut(64)\n",
    "    .build())\n",
    "  .layer(7, new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX)\n",
    "    .kernelSize(3, 3)\n",
    "    .name(\"pool3\")\n",
    "    .build())\n",
    "  .layer(8, new DenseLayer.Builder()\n",
    "    .name(\"ffn1\")\n",
    "    .nOut(250)\n",
    "    .dropOut(0.5)\n",
    "    .build())\n",
    "  .layer(9, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "    .nOut(outputNum)\n",
    "    .activation(\"softmax\")\n",
    "    .build())\n",
    "  .backprop(true).pretrain(false)\n",
    "  .cnnInputSize(height, width, channels).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***AlexNet Example***<br>\n",
    " The second configuration is a slight variant on AlexNet which won the ImageNet competition in 2012 for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// AlexNet Example\n",
    "val nonZeroBias = 1\n",
    "val dropOut = 0.5\n",
    "val poolingType: SubsamplingLayer.PoolingType = SubsamplingLayer.PoolingType.MAX\n",
    "\n",
    "val confAlexNet: MultiLayerConfiguration = new NeuralNetConfiguration.Builder()\n",
    "    .seed(seed)\n",
    "    .weightInit(WeightInit.XAVIER)\n",
    "    .activation(\"relu\")\n",
    "    .updater(Updater.NESTEROVS)\n",
    "    .iterations(iterations)\n",
    "    // normalize to prevent vanishing or exploding gradients\n",
    "    .gradientNormalization(GradientNormalization.RenormalizeL2PerLayer) \n",
    "    .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)\n",
    "    .learningRate(1e-3)\n",
    "    .learningRateScoreBasedDecayRate(1e-1)\n",
    "    .regularization(true)\n",
    "    .l2(5 * 1e-4)\n",
    "    .momentum(0.9)\n",
    "    .miniBatch(false)\n",
    "    .list()\n",
    "            //conv1\n",
    "    .layer(0, new ConvolutionLayer.Builder(new int[]{11, 11}, new int[]{4, 4}, new int[]{3, 3})\n",
    "            .name(\"cnn1\")\n",
    "            .nIn(channels)\n",
    "            .nOut(96)\n",
    "            .build())\n",
    "    .layer(1, new LocalResponseNormalization.Builder()\n",
    "            .name(\"lrn1\")\n",
    "            .build())\n",
    "    .layer(2, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})\n",
    "            .name(\"pool1\")\n",
    "            .build())\n",
    "            //conv2\n",
    "    .layer(3, new ConvolutionLayer.Builder(new int[]{5, 5}, new int[]{1, 1}, new int[]{2, 2})\n",
    "            .name(\"cnn2\")\n",
    "            .nOut(256)\n",
    "            .biasInit(nonZeroBias)\n",
    "            .build())\n",
    "    .layer(4, new LocalResponseNormalization.Builder()\n",
    "            .name(\"lrn2\")\n",
    "            .k(2).n(5).alpha(1e-4).beta(0.75)\n",
    "            .build())\n",
    "    .layer(5, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})\n",
    "            .name(\"pool2\")\n",
    "            .build())\n",
    "            //conv3\n",
    "    .layer(6, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})\n",
    "            .name(\"cnn3\")\n",
    "            .nOut(384)\n",
    "            .build())\n",
    "            //conv4\n",
    "    .layer(7, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})\n",
    "            .name(\"cnn4\")\n",
    "            .nOut(384)\n",
    "            .biasInit(nonZeroBias)\n",
    "            .build())\n",
    "            //conv5\n",
    "    .layer(8, new ConvolutionLayer.Builder(new int[]{3, 3}, new int[]{1, 1}, new int[]{1, 1})\n",
    "            .name(\"cnn5\")\n",
    "            .nOut(256)\n",
    "            .biasInit(nonZeroBias)\n",
    "            .build())\n",
    "    .layer(9, new SubsamplingLayer.Builder(poolingType, new int[]{3, 3}, new int[]{2, 2})\n",
    "            .name(\"pool3\")\n",
    "            .build())\n",
    "    .layer(10, new DenseLayer.Builder()\n",
    "            .name(\"ffn1\")\n",
    "            .nOut(4096)\n",
    "            .biasInit(nonZeroBias)\n",
    "            .dropOut(dropOut)\n",
    "            .build())\n",
    "    .layer(11, new DenseLayer.Builder()\n",
    "            .name(\"ffn2\")\n",
    "            .nOut(4096)\n",
    "            .biasInit(nonZeroBias)\n",
    "            .dropOut(dropOut)\n",
    "            .build())\n",
    "    .layer(12, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n",
    "            .name(\"output\")\n",
    "            .nOut(outputNum)\n",
    "            .activation(\"softmax\")\n",
    "            .build())\n",
    "    .backprop(true)\n",
    "    .pretrain(false)\n",
    "    .cnnInputSize(height,width,channels).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Initialize the network and alternate which configuration to pass into MultiLayerNetwork\n",
    "val network: MultiLayerNetwork = new MultiLayerNetwork(confAlexNet)\n",
    "network.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Listeners***\n",
    "\n",
    "Apply setListeners to the network to get information on how the model is performing. ScoreIterationListener is the simplest one to check if the model is converging in its predictions on the training data. Basically its showing how accurate is the model predicting the results of the training data. Typically you are working to lower the scores as close to zero as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.setListeners(new ScoreIterationListener(listenerFreq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Gradients***\n",
    "\n",
    "Backpropagation is how you move the weight($w$) updates from stochastic gradient descent back into the model. Sometimes there are score results of NaN or 0 because the gradient explodes or vanishes. As changes are moved backwards through the layers in deep nets, the gradient tends to get smaller. The neurons in the beginning layers learn more slowly than the neurons in the later layers which can make it vanish. Sometimes the gradient gets too big in earlier layers which makes it explode. More information on how to address these issues are in the references below. Just be aware this is common and requires tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've loaded the data and the model configuration is initialized, train the model by calling fit on the configured network and passing in the dataset. The goal of training is to define parameters that will provide high accuracy on classification results but generalize enough to perform well on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Runs 1 epoch\n",
    "val testInput = new ListBuffer[INDArray]()\n",
    "val testLabels = new ListBuffer[INDArray]()\n",
    "\n",
    "while (dataIter.hasNext()) {\n",
    "  val dsNext: DataSet = dataIter.next()\n",
    "  dsNext.scale()\n",
    "  val trainTest: SplitTestAndTrain = dsNext.splitTestAndTrain(splitTrainNum, new Random(seed))\n",
    "  val trainInput: DataSet = trainTest.getTrain() // get feature matrix and labels for training\n",
    "  testInput += trainTest.getTest().getFeatureMatrix()\n",
    "  testLabels += trainTest.getTest().getLabels()\n",
    "  network.fit(trainInput)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Assumes 1 epoch completed already\n",
    "for (i <- 1 until epochs) {\n",
    "  dataIter.reset()\n",
    "  while (dataIter.hasNext()) {\n",
    "    val dsNext: DataSet = dataIter.next()\n",
    "    val trainTest: SplitTestAndTrain = dsNext.splitTestAndTrain(splitTrainNum, new Random(seed))\n",
    "    val trainInput: DataSet = trainTest.getTrain()\n",
    "    network.fit(trainInput)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model converges in regards to its loss function, you can run new test data through the model to see how well it generalizes and predicts. The test data should be a dataset that was not used during training.\n",
    "\n",
    "Example performance indicators:\n",
    "- ***accuracy*** = number of correct predictions to total predictions \n",
    "- ***precision*** = number of correct positive predictions divided by total positive class values predicted\n",
    "- ***recall*** = number of correct positive predictions divided by the total actual positive class values\n",
    "- ***f1-score*** = measure of test accuracy as a balance between precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val eval: Evaluation = new Evaluation(labels)\n",
    "while (dataIter.hasNext()) {\n",
    "  val testDS: DataSet = dataIter.next(batchSize)\n",
    "  val output: INDArray = network.output(testDS.getFeatureMatrix())\n",
    "  eval.eval(testDS.getLabels(), output)\n",
    "}\n",
    "print(eval.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model configuration and parameters when you are satisfied with evaluation scores or if a training break is needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val basePath = FilenameUtils.concat(System.getProperty(\"user.dir\"))\n",
    "val confPath = FilenameUtils.concat(basePath, network.toString() + \"-conf.json\")\n",
    "val paramPath = FilenameUtils.concat(basePath, network.toString() + \".bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Save parameters\n",
    "try {\n",
    "  val dos: DataOutputStream = new DataOutputStream(new FileOutputStream(paramPath))\n",
    "  Nd4j.write(network.params(), dos)\n",
    "  dos.flush()\n",
    "  dos.close()\n",
    "  // Save model configuration\n",
    "  FileUtils.write(new File(confPath), network.conf().toJson())\n",
    "} catch {\n",
    "  case ioe: IOException => ioe.printStackTrace()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to loading data and the time to train, tuning is a one of the key challenges to produce effective neural nets. To get a good sense of how to tune, spend time running different models and reading academic papers that outline various approaches. This will help you gain understanding of how to tune. Below are a couple pointers to get you started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***General Pointers***\n",
    "\n",
    "Start with as few hyper-parameters as possible to start and focus on improving scores with those first. Also, focus on tuning one hyper-parameter at a time and keep the others fixed. When it seems you can no longer improve the scores on it, change to a new one and be willing to go back to the first after you've made adjustments to other hyper-parameters. \n",
    "\n",
    "***Learning Rate ( $\\alpha$)***\n",
    "\n",
    "Learning rate is a good hyper-parameter to start with. Watch how the scores change and if it is a smooth decrease till the final epoch that's a good parameter to work with. If it's smooth early on and then oscillates randomly or if the scores climb then lower the parameter. Shift by order magnitude like 10 and then make the adjustments smaller as you get closer to a smooth decrease.\n",
    "\n",
    "***Mini-batch Size***\n",
    "\n",
    "Mini-batch size makes a difference when tuning. If its too small then you aren't maximing matrix library optimizations and too large leads to not updatig the weights enough. Be aware that the size is independent of other hyper-parameters so you don't have to have tuned hyper-parameters to find a good mini-batch size. Look for accuracy vs time to find the size that works best.\n",
    "\n",
    "***Batch Normalization***\n",
    "\n",
    "Batch normalization is the popular technique in the last year for deep neural net training because it leads to faster learning and higher overall accuracy. You can work with higher learning rates and avoid using regularization techniques like dropout. When passing in input, it is common to scale the input by shifting it to zero-mean and unit variance but as the input passes through the net it gets adjusted by parameters which is known as \"covariate shift\". Using batch norm in each mini-batch and between layers helps to reset the input normalization.\n",
    "\n",
    "***Automated Tuning***\n",
    "\n",
    "Manual tuning is great to get a feel how to use hyper-parameters but when you want to get quick results, automated tuning techniques will help cut down training time. There are many different approaches to try like grid, random and bayesian. \n",
    "\n",
    "\n",
    "For more information in general on tuning check out the references below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've spent time training and tuning the net, you should end up with a configuration and parameters you can apply to new datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For more information on how to develop neural nets, below are additional resources to explore.\n",
    "\n",
    "- Skymind: http://www.skymind.io/\n",
    "- U.S. Fish and Wildlife Service (animal sample dataset): http://digitalmedia.fws.gov/cdm/\n",
    "- Tiny ImageNet Classification with CNN: http://cs231n.stanford.edu/reports/leonyao_final.pdf\n",
    "- AlexNet: http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf & https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt\n",
    "- Neural Networks and Deep Learning: http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "- Neuarl Networks: http://nbviewer.jupyter.org/github/masinoa/machine_learning/blob/master/04_Neural_Networks.ipynb\n",
    "- Visual Information Theory: https://colah.github.io/posts/2015-09-Visual-Information/\n",
    "- Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift: http://jmlr.org/proceedings/papers/v37/ioffe15.pdf\n",
    "- Deep Learning Booke: http://www.deeplearningbook.org/\n",
    "- Neural Networks for Machine Learning: https://www.coursera.org/course/neuralnets\n",
    "- Convolutional Neural Networks for Visual Recognition: http://cs231n.github.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala210",
   "name": "scala210"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": "scala",
   "mimetype": "text/x-scala",
   "name": "scala210",
   "pygments_lexer": "scala",
   "version": "2.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
